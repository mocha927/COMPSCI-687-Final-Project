{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import ale_py\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\") \n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (torch.FloatTensor(np.float32(state)), \n",
    "                torch.LongTensor(action), \n",
    "                torch.FloatTensor(reward), \n",
    "                torch.FloatTensor(np.float32(next_state)),\n",
    "                torch.BoolTensor(done))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        \n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-4)\n",
    "        self.memory = ReplayBuffer(100000)\n",
    "        \n",
    "        self.batch_size = 1024\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.02\n",
    "        self.eps_decay = 100000\n",
    "        self.target_update = 1000\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "        else:\n",
    "            return random.randrange(self.n_actions)\n",
    "    \n",
    "    def select_greedy_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        state_batch = state_batch.to(self.device)\n",
    "        action_batch = action_batch.to(self.device)\n",
    "        reward_batch = reward_batch.to(self.device)\n",
    "        next_state_batch = next_state_batch.to(self.device)\n",
    "        done_batch = done_batch.to(self.device)\n",
    "        \n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_state_batch).max(1)[0]\n",
    "            next_state_values[done_batch] = 0.0\n",
    "            expected_q_values = reward_batch + self.gamma * next_state_values\n",
    "        \n",
    "        loss = nn.SmoothL1Loss()(current_q_values.squeeze(), expected_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = frame[35:195]  # Crop\n",
    "    frame = frame[::2, ::2]  # Downsample\n",
    "    frame = frame.mean(axis=2)  # Convert to grayscale\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "def train(agent, env, num_episodes=1000):\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        frame, _ = env.reset()\n",
    "        frame = preprocess_frame(frame)\n",
    "        \n",
    "        for _ in range(4):\n",
    "            frame_stack.append(frame)\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            state = np.array(frame_stack)\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_frame, reward, done, truncated, _ = env.step(action)\n",
    "            next_frame = preprocess_frame(next_frame)\n",
    "            frame_stack.append(next_frame)\n",
    "            next_state = np.array(frame_stack)\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, bool(done))\n",
    "            \n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                loss = agent.optimize_model()\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if agent.steps_done % agent.target_update == 0:\n",
    "                agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Steps: {agent.steps_done}\")\n",
    "        losses.append(total_reward)\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % 100 == 99:\n",
    "            torch.save(agent.policy_net.state_dict(), f\"pong_dqn_episode_{episode}.pth\")\n",
    "\n",
    "        if episode == num_episodes - 1:\n",
    "            torch.save(agent.policy_net.state_dict(), f\"pong_dqn_episode_end.pth\")\n",
    "\n",
    "    return agent, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = (4, 80, 80)  # 4 frames stacked, 80x80 each\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_shape, n_actions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -20.0, Steps: 842\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "if os.path.exists(\"pong_dqn_episode_end.pth\"):\n",
    "    agent.policy_net.load_state_dict(torch.load(\"pong_dqn_episode_end.pth\"))\n",
    "    losses = np.load(\"losses.npy\")\n",
    "else:\n",
    "    agent, losses = train(agent, env, num_episodes=1000)\n",
    "    np.save(\"losses.npy\", np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent, env):\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    frame, _ = env.reset()\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        frame_stack.append(frame)\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        state = np.array(frame_stack)\n",
    "        action = agent.select_greedy_action(state)\n",
    "        \n",
    "        next_frame, reward, done, truncated, _ = env.step(action)\n",
    "        next_frame = preprocess_frame(next_frame)\n",
    "        frame_stack.append(next_frame)\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward per Episode vs Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_game(agent):\n",
    "    from IPython import display\n",
    "    plt.ion()\n",
    "\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    frame, _ = env.reset()\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        frame_stack.append(frame)\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        state = np.array(frame_stack)\n",
    "        action = agent.select_greedy_action(state)\n",
    "        \n",
    "        next_frame, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(next_frame)\n",
    "        plt.axis('off')\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()\n",
    "        \n",
    "        next_frame = preprocess_frame(next_frame)\n",
    "        frame_stack.append(next_frame)\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_game(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
